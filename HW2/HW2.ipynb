{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Read CSV text file\n",
    "dev = [s.strip().split(\", \") for s in open('hw1-data/income.dev.txt').readlines()]\n",
    "\n",
    "train = [s.strip().split(\", \") for s in open('hw1-data/income.train.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensionality: 230\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# IMPORT DATA FROM LAST WEEKS SOLUTION\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def process_data(filename):\n",
    "    X, Y = [], []\n",
    "    for j, line in enumerate(open(filename)):\n",
    "        line = line.strip()\n",
    "        features = line.split(\", \")\n",
    "        feat_vec = np.zeros(dimension)\n",
    "        for i, fv in enumerate(features[:-1]): # last one is target\n",
    "            if i in numerical_fields: # two numerical fields\n",
    "                feat_vec[feature_map[i, 0]] = float(fv) / 50  # NB: diff 2 not 1!\n",
    "            elif (i, fv) in feature_map: # ignore unobserved features\n",
    "                feat_vec[feature_map[i, fv]] = 1\n",
    "\n",
    "        X.append(feat_vec)\n",
    "\n",
    "        Y.append(1 if features[-1].upper() == \">50K\" else -1) # fake for testdata\n",
    "    \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "\n",
    "field_value_freqs = defaultdict(lambda : defaultdict(int)) # field_id -> value -> freq\n",
    "numerical_fields = [] # [] for binarizing all the fields\n",
    "for line in open(\"hw1-data/income.train.txt\"):\n",
    "    line = line.strip()\n",
    "    features = line.split(\", \")[:-1] # exclude target label\n",
    "    for i, fv in enumerate(features):\n",
    "        field_value_freqs[i][0 if i in numerical_fields else fv] += 1\n",
    "\n",
    "feature_map = {}\n",
    "feature_remap = {}\n",
    "for i, value_freqs in field_value_freqs.items():\n",
    "    for v in value_freqs:\n",
    "        k = len(feature_map) # bias\n",
    "        feature_map[i, v] = k\n",
    "        feature_remap[k] = i, v\n",
    "\n",
    "dimension = len(feature_map) # bias\n",
    "print(\"dimensionality: %d\" % dimension) #, feature_map\n",
    "\n",
    "train_data = process_data(\"hw1-data/income.train.txt\")\n",
    "dev_data = process_data(\"hw1-data/income.dev.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla\n",
      "epoch 1 updates 1257 (26.48%) dev_err 22.4% (+:27.5%)\n",
      "epoch 2 updates 1221 (26.64%) dev_err 20.8% (+:25.4%)\n",
      "epoch 3 updates 1177 (26.22%) dev_err 18.0% (+:21.5%)\n",
      "epoch 4 updates 1170 (26.2%) dev_err 19.7% (+:12.3%)\n",
      "epoch 5 updates 1172 (26.22%) dev_err 18.7% (+:17.7%)\n",
      "\n",
      "Best Dev Error: 18.0%\n",
      "\n",
      "Average\n",
      "epoch 1 updates 1257 (26.48%) dev_err 15.0% (+:18.6%)\n",
      "epoch 2 updates 1221 (26.64%) dev_err 15.1% (+:19.3%)\n",
      "epoch 3 updates 1177 (26.22%) dev_err 14.8% (+:20.0%)\n",
      "epoch 4 updates 1170 (26.2%) dev_err 14.7% (+:19.3%)\n",
      "epoch 5 updates 1172 (26.22%) dev_err 14.8% (+:20.0%)\n",
      "\n",
      "Best Dev Error: 14.7%\n",
      "duration is: 0.27571988105773926 seconds.\n"
     ]
    }
   ],
   "source": [
    "# cleaning\n",
    "\n",
    "## HOW DO I IMPLEMENT AVERAGE?\n",
    "\n",
    "# DEVELOP DATA TO PASS TO perc() FUNCTION\n",
    "data_train_tupe = []\n",
    "for i, row in enumerate(zip(train_data[0],train_data[1])):\n",
    "    row_vect = np.append(row[0],[1.]) # ADD STUFF TO VECTOR HERE\n",
    "    data_train_tupe.append((row_vect,row[1])) \n",
    "    \n",
    "data_dev_tupe = []\n",
    "for i, row in enumerate(zip(dev_data[0],dev_data[1])):\n",
    "    row_vect = np.append(row[0],[1.]) # ADD STUFF TO VECTOR HERE\n",
    "    data_dev_tupe.append((row_vect,row[1]))\n",
    "    \n",
    "# ADD BIAS BY ADDING 1 AT END OF BINARIZED VECTOR\n",
    "\n",
    "# TRAIN MODEL\n",
    "def perc(data, dev_data, method = 'vanilla'):\n",
    "    weight = np.zeros(len(data[0][0]))\n",
    "    weight_s = np.zeros(len(data[0][0]))\n",
    "    avgw = np.array([0.,0.,0.])\n",
    "    epochs = 5\n",
    "    supp_vec = set()\n",
    "    best_margin = 10000\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        k = 0\n",
    "        error = 0\n",
    "        match = 0\n",
    "        positives = 0\n",
    "        #tavgw = np.array([0.,0.,0.])\n",
    "        \n",
    "        for i, (x, y) in enumerate(data):\n",
    "            \n",
    "            prediction = np.dot(x,weight)\n",
    "            \n",
    "            # POSITIVES\n",
    "            if (prediction >= 0): #and y >= 0) :\n",
    "                positives += 1\n",
    "                \n",
    "            # POSITIVE MATCH\n",
    "            if (prediction >= 0 and y >= 0) :\n",
    "                match += 1\n",
    "                \n",
    "            # NEG MATCH\n",
    "            elif (prediction < 0 and y < 0):\n",
    "                match += 1\n",
    "                \n",
    "            # NO MATCH\n",
    "            else:\n",
    "                error += 1\n",
    "            \n",
    "            if prediction * y <= 0:\n",
    "                weight += x * y\n",
    "                k += 1\n",
    "                \n",
    "            # update ws (AVERAGE PERCEPTRON METHOD)\n",
    "            weight_s += weight \n",
    "            \n",
    "        if method == 'vanilla':\n",
    "            dev_err, dev_pos = test_perc(weight, dev_data) # CHANGE PASS FROM WEIGHT TO WEIGHT_S FOR AVG VS NO AVERAGE\n",
    "\n",
    "        else:\n",
    "            dev_err, dev_pos = test_perc(weight_s, dev_data) # CHANGE PASS FROM WEIGHT TO WEIGHT_S FOR AVG VS NO AVERAGE\n",
    "\n",
    "        \n",
    "        print('epoch', e+1, 'updates', k, f'({round((positives/len(data)*100),2)}%)', 'dev_err', f'{round(dev_err,2)}%', f'(+:{round(dev_pos,2)}%)')\n",
    "        \n",
    "        if dev_err < best_margin:\n",
    "            best_margin = dev_err\n",
    "            best_weight = weight_s\n",
    "        \n",
    "    print('\\nBest Dev Error:', f'{round(best_margin,2)}%')\n",
    "    return best_weight\n",
    "        \n",
    "          \n",
    "# TEST MODEL\n",
    "def test_perc(weights_s, df_bin, phase = 'normal'):\n",
    "    '''takes weights previously calculated and applies them to binarized dataframe. Then calculates positive percent and error rates'''        \n",
    "    k = 0\n",
    "    error = 0\n",
    "    match = 0\n",
    "    positives = 0\n",
    "      \n",
    "    for i, (x, y) in enumerate(df_bin):\n",
    "          \n",
    "        prediction = np.dot(x,weights_s)\n",
    "            \n",
    "        # POSITIVES\n",
    "        if (prediction > 0): #and y >= 0) :\n",
    "            positives += 1\n",
    "                \n",
    "        # POSITIVE MATCH\n",
    "        if (prediction >= 0 and y >= 0) :\n",
    "            match += 1\n",
    "                \n",
    "        # NEG MATCH\n",
    "        elif (prediction < 0 and y < 0):\n",
    "            match += 1\n",
    "                \n",
    "        # NO MATCH\n",
    "        else:\n",
    "            error += 1\n",
    "        \n",
    "        \n",
    "    dev_err = error/len(df_bin)*100\n",
    "    dev_pos = positives/len(df_bin)*100\n",
    "    \n",
    "    return dev_err, dev_pos\n",
    "        \n",
    "# CALL PERC FUNCTION        \n",
    "\n",
    "time1 = time.time()\n",
    "\n",
    "print('Vanilla')\n",
    "best_weight_van = perc(data_train_tupe, data_dev_tupe)\n",
    "\n",
    "print('\\nAverage')\n",
    "best_weight_avg = perc(data_train_tupe, data_dev_tupe, method = 'average')\n",
    "\n",
    "time2 = time.time()\n",
    "print('duration is:', time2-time1, 'seconds.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Positive Index: 85 Positive Weight: 178980.0 Positive Feature: (2, 'Doctorate')\n",
      "Neg Index: 3 Neg Weight: -150069.0 Neg Feature: (0, '28')\n",
      "\n",
      "Positive Index: 90 Positive Weight: 151494.0 Positive Feature: (3, 'Married-civ-spouse')\n",
      "Neg Index: 230 Neg Weight: -146333.0 Neg Feature: (3, 'Married-civ-spouse')\n",
      "\n",
      "Positive Index: 83 Positive Weight: 137016.0 Positive Feature: (2, 'Prof-school')\n",
      "Neg Index: 81 Neg Weight: -141763.0 Neg Feature: (2, '7th-8th')\n",
      "\n",
      "Positive Index: 201 Positive Weight: 121924.0 Positive Feature: (8, 'Iran')\n",
      "Neg Index: 104 Neg Weight: -140683.0 Neg Feature: (4, 'Farming-fishing')\n",
      "\n",
      "Positive Index: 148 Positive Weight: 102122.0 Positive Feature: (7, '65')\n",
      "Neg Index: 46 Neg Weight: -124170.0 Neg Feature: (0, '26')\n"
     ]
    }
   ],
   "source": [
    "# CALCUALTE TOP AND BOTTOM\n",
    "\n",
    "top_bott = []\n",
    "for i, w in enumerate(best_weight_avg):\n",
    "    top_bott.append((i,w))\n",
    "    \n",
    "top_bott_sorted = sorted(top_bott, key = lambda x: x[1], reverse = True)\n",
    "for i in range(5):\n",
    "    # POSITIVE\n",
    "    for key, vals in feature_map.items():\n",
    "        if vals == top_bott_sorted[i][0]:\n",
    "            feat_val = key\n",
    "    print('\\nPositive Index:', top_bott_sorted[i][0], 'Positive Weight:', top_bott_sorted[i][1], 'Positive Feature:', feat_val)\n",
    "    \n",
    "    neg_i = -i -1\n",
    "    # NEGATIVE\n",
    "    for key, vals in feature_map.items():\n",
    "        if vals == top_bott_sorted[neg_i][0]:\n",
    "            feat_val = key\n",
    "    print('Neg Index:', top_bott_sorted[neg_i][0], 'Neg Weight:', top_bott_sorted[neg_i][1], 'Neg Feature:', feat_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redo using new, ordered data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE ORDERED CSV \n",
    "train_rev = sorted(train, key=lambda x: x[-1]) \n",
    "\n",
    "np.savetxt('train_ordered.txt', train_rev, delimiter = ', ', fmt = '% s')\n",
    "\n",
    "train_data_new = process_data(\"train_ordered.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla\n",
      "epoch 1 updates 4 (25.02%) dev_err 76.4% (+:100.0%)\n",
      "epoch 2 updates 8 (25.02%) dev_err 76.4% (+:100.0%)\n",
      "epoch 3 updates 9 (25.04%) dev_err 76.3% (+:98.9%)\n",
      "epoch 4 updates 9 (25.06%) dev_err 76.4% (+:100.0%)\n",
      "epoch 5 updates 10 (25.08%) dev_err 76.3% (+:99.7%)\n",
      "\n",
      "Best Dev Error: 76.3%\n",
      "\n",
      "Average\n",
      "epoch 1 updates 4 (25.02%) dev_err 27.8% (+:4.4%)\n",
      "epoch 2 updates 8 (25.02%) dev_err 23.7% (+:0.3%)\n",
      "epoch 3 updates 9 (25.04%) dev_err 23.6% (+:0.0%)\n",
      "epoch 4 updates 9 (25.06%) dev_err 23.6% (+:0.0%)\n",
      "epoch 5 updates 10 (25.08%) dev_err 23.6% (+:0.0%)\n",
      "\n",
      "Best Dev Error: 23.6%\n"
     ]
    }
   ],
   "source": [
    "data_train_tupe_new = []\n",
    "for i, row in enumerate(zip(train_data_new[0],train_data_new[1])):\n",
    "    row_vect = np.append(row[0],[1.]) # ADD STUFF TO VECTOR HERE\n",
    "    data_train_tupe_new.append((row_vect,row[1])) \n",
    "    \n",
    "\n",
    "data_dev_tupe_new = []\n",
    "for i, row in enumerate(zip(dev_data[0],dev_data[1])):\n",
    "    row_vect = np.append(row[0],[1.]) # ADD STUFF TO VECTOR HERE\n",
    "    data_dev_tupe_new.append((row_vect,row[1]))\n",
    "    \n",
    "    \n",
    "print('Vanilla')\n",
    "weight = perc(data_train_tupe_new, data_dev_tupe_new)\n",
    "\n",
    "print('\\nAverage')\n",
    "weight = perc(data_train_tupe_new, data_dev_tupe_new, method = 'Average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla\n",
      "epoch 1 updates 1858 (27.42%) dev_err 23.8% (+:0.2%)\n",
      "epoch 2 updates 1676 (26.54%) dev_err 23.7% (+:0.1%)\n",
      "epoch 3 updates 1601 (26.24%) dev_err 18.6% (+:23.8%)\n",
      "epoch 4 updates 1516 (26.02%) dev_err 19.6% (+:26.8%)\n",
      "epoch 5 updates 1510 (25.9%) dev_err 23.4% (+:0.2%)\n",
      "\n",
      "Best Dev Error: 18.6%\n",
      "\n",
      "Average\n",
      "epoch 1 updates 1858 (27.42%) dev_err 23.6% (+:0.0%)\n",
      "epoch 2 updates 1676 (26.54%) dev_err 23.6% (+:0.0%)\n",
      "epoch 3 updates 1601 (26.24%) dev_err 23.6% (+:0.0%)\n",
      "epoch 4 updates 1516 (26.02%) dev_err 23.5% (+:0.1%)\n",
      "epoch 5 updates 1510 (25.9%) dev_err 22.7% (+:1.5%)\n",
      "\n",
      "Best Dev Error: 22.7%\n"
     ]
    }
   ],
   "source": [
    "# ADD ORIGINAL AGE AND HOURS\n",
    "data_train_tupe_feature1 = []\n",
    "for i, row in enumerate(zip(train_data[0],train_data[1])):\n",
    "    row_vect = np.append(row[0], [1., float(train[i][0]), float(train[i][-3])]) # ADD STUFF TO VECTOR HERE\n",
    "    data_train_tupe_feature1.append((row_vect,row[1])) \n",
    "    \n",
    "data_dev_tupe_feature1 = []\n",
    "for i, row in enumerate(zip(dev_data[0],dev_data[1])):\n",
    "    row_vect = np.append(row[0], [1., float(dev[i][0]), float(dev[i][-3])]) # ADD STUFF TO VECTOR HERE\n",
    "    data_dev_tupe_feature1.append((row_vect,row[1]))\n",
    "\n",
    "\n",
    "print('Vanilla')\n",
    "weight = perc(data_train_tupe_feature1, data_dev_tupe_feature1)\n",
    "\n",
    "print('\\nAverage')\n",
    "weight = perc(data_train_tupe_feature1, data_dev_tupe_feature1, method = 'Average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla\n",
      "epoch 1 updates 1177 (25.08%) dev_err 22.9% (+:30.1%)\n",
      "epoch 2 updates 1121 (25.04%) dev_err 20.9% (+:25.1%)\n",
      "epoch 3 updates 1150 (25.02%) dev_err 22.0% (+:26.0%)\n",
      "epoch 4 updates 1107 (25.0%) dev_err 25.3% (+:33.7%)\n",
      "epoch 5 updates 1129 (25.04%) dev_err 23.6% (+:25.4%)\n",
      "\n",
      "Best Dev Error: 20.9%\n",
      "\n",
      "Average\n",
      "epoch 1 updates 1177 (25.08%) dev_err 14.1% (+:20.1%)\n",
      "epoch 2 updates 1121 (25.04%) dev_err 14.5% (+:20.5%)\n",
      "epoch 3 updates 1150 (25.02%) dev_err 14.5% (+:21.1%)\n",
      "epoch 4 updates 1107 (25.0%) dev_err 14.8% (+:21.0%)\n",
      "epoch 5 updates 1129 (25.04%) dev_err 15.4% (+:21.4%)\n",
      "\n",
      "Best Dev Error: 14.1%\n"
     ]
    }
   ],
   "source": [
    "# CONVERT TO ZERO MEAN\n",
    "\n",
    "train_mean = np.mean(train_data[0], axis = 0)\n",
    "\n",
    "\n",
    "data_train_tupe_mean = []\n",
    "for i, row in enumerate(zip(train_data[0] ,train_data[1])):\n",
    "    row_vect = np.append(row[0]- train_mean,[1.]) # ADD STUFF TO VECTOR HERE\n",
    "    data_train_tupe_mean.append((row_vect,row[1])) \n",
    "\n",
    "data_dev_tupe_mean = []\n",
    "for i, row in enumerate(zip(dev_data[0],dev_data[1])):\n",
    "    row_vect = np.append(row[0] - train_mean,[1.]) # ADD STUFF TO VECTOR HERE\n",
    "    data_dev_tupe_mean.append((row_vect,row[1]))\n",
    "        \n",
    "    \n",
    "print('Vanilla')\n",
    "weight_s = perc(data_train_tupe_mean, data_dev_tupe_mean)\n",
    "\n",
    "print('\\nAverage')\n",
    "best_weight_mean = perc(data_train_tupe_mean, data_dev_tupe_mean, method = 'average')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla\n",
      "epoch 1 updates 1162 (25.24%) dev_err 22.8% (+:26.6%)\n",
      "epoch 2 updates 1091 (25.0%) dev_err 23.7% (+:28.9%)\n",
      "epoch 3 updates 1134 (25.06%) dev_err 23.3% (+:29.1%)\n",
      "epoch 4 updates 1128 (24.98%) dev_err 25.3% (+:35.9%)\n",
      "epoch 5 updates 1119 (25.08%) dev_err 20.2% (+:19.2%)\n",
      "\n",
      "Best Dev Error: 20.2%\n",
      "\n",
      "Average\n",
      "epoch 1 updates 1162 (25.24%) dev_err 16.5% (+:21.3%)\n",
      "epoch 2 updates 1091 (25.0%) dev_err 16.2% (+:20.6%)\n",
      "epoch 3 updates 1134 (25.06%) dev_err 15.8% (+:21.2%)\n",
      "epoch 4 updates 1128 (24.98%) dev_err 16.1% (+:21.1%)\n",
      "epoch 5 updates 1119 (25.08%) dev_err 15.7% (+:21.5%)\n",
      "\n",
      "Best Dev Error: 15.7%\n"
     ]
    }
   ],
   "source": [
    "# STANDARD DEVIATION\n",
    "train_std = np.std(train_data[0], axis=0)\n",
    "\n",
    "\n",
    "data_train_std = []\n",
    "for i, row in enumerate(zip(train_data[0],train_data[1])):\n",
    "    row_vect = np.append(row[0]/train_std,[1.]) # ADD STUFF TO VECTOR HERE\n",
    "    data_train_std.append((row_vect,row[1])) \n",
    "\n",
    "    \n",
    "data_dev_tupe_std = []\n",
    "for i, row in enumerate(zip(dev_data[0],dev_data[1])):\n",
    "    row_vect = np.append(row[0]/train_std,[1.]) # ADD STUFF TO VECTOR HERE\n",
    "    data_dev_tupe_std.append((row_vect,row[1]))\n",
    "    \n",
    "    \n",
    "    \n",
    "print('Vanilla')\n",
    "weight_s = perc(data_train_std, data_dev_tupe_std)\n",
    "\n",
    "print('\\nAverage')\n",
    "weight_s = perc(data_train_std, data_dev_tupe_std, method = 'average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla\n",
      "epoch 1 updates 1546 (26.14%) dev_err 23.1% (+:0.7%)\n",
      "epoch 2 updates 1295 (25.32%) dev_err 17.6% (+:28.0%)\n",
      "epoch 3 updates 1210 (25.18%) dev_err 16.0% (+:22.8%)\n",
      "epoch 4 updates 1186 (25.22%) dev_err 19.0% (+:6.6%)\n",
      "epoch 5 updates 1158 (25.14%) dev_err 19.6% (+:34.4%)\n",
      "\n",
      "Best Dev Error: 16.0%\n",
      "\n",
      "Average\n",
      "epoch 1 updates 1546 (26.14%) dev_err 18.1% (+:9.3%)\n",
      "epoch 2 updates 1295 (25.32%) dev_err 16.6% (+:13.6%)\n",
      "epoch 3 updates 1210 (25.18%) dev_err 16.1% (+:15.5%)\n",
      "epoch 4 updates 1186 (25.22%) dev_err 16.2% (+:17.0%)\n",
      "epoch 5 updates 1158 (25.14%) dev_err 15.8% (+:18.2%)\n",
      "\n",
      "Best Dev Error: 15.8%\n"
     ]
    }
   ],
   "source": [
    "# COMBINATION\n",
    "train_std = np.std(train_data[0], axis=0)\n",
    "\n",
    "\n",
    "data_train_test = []\n",
    "for i, row in enumerate(zip(train_data[0],train_data[1])):\n",
    "    row_vect = np.append(row[0]/train_std,[1., float(train[i][0]), float(train[i][-3])]) # ADD STUFF TO VECTOR HERE\n",
    "    data_train_test.append((row_vect,row[1])) \n",
    "\n",
    "    \n",
    "data_dev_tupe_test = []\n",
    "for i, row in enumerate(zip(dev_data[0],dev_data[1])):\n",
    "    row_vect = np.append(row[0]/train_std,[1., float(dev[i][0]), float(dev[i][-3])]) # ADD STUFF TO VECTOR HERE\n",
    "    data_dev_tupe_test.append((row_vect,row[1]))\n",
    "    \n",
    "    \n",
    "    \n",
    "print('Vanilla')\n",
    "weight_s = perc(data_train_test, data_dev_tupe_test)\n",
    "\n",
    "print('\\nAverage')\n",
    "weight_s = perc(data_train_test, data_dev_tupe_test, method = 'average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Blind Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new feature: (7, '31') cannot be analysed. I\"m OK with that.\n",
      "new feature: (0, '83') cannot be analysed. I\"m OK with that.\n",
      "new feature: (0, '82') cannot be analysed. I\"m OK with that.\n",
      "new feature: (7, '29') cannot be analysed. I\"m OK with that.\n",
      "new feature: (0, '82') cannot be analysed. I\"m OK with that.\n",
      "new feature: (8, 'Hungary') cannot be analysed. I\"m OK with that.\n",
      "79.60000000000001 20.4\n"
     ]
    }
   ],
   "source": [
    "blind = [s.strip().split(\", \") for s in open('hw1-data/income_blind.txt').readlines()]\n",
    "\n",
    "new_data = []\n",
    "\n",
    "best_weight_mean = best_weight_mean # DECLARED UP ABOVE USING AVERAGED METHOD EPOCH 1 WEIGHT\n",
    "\n",
    "\n",
    "#BINARIZE Blind\n",
    "for row in blind:\n",
    "    new_row = []\n",
    "    for j, x in enumerate(row):\n",
    "        feature = (j, x)\n",
    "        try:\n",
    "            new_row.append(feature_map[feature])\n",
    "        except:\n",
    "            print('new feature:', feature, 'cannot be analysed. I\"m OK with that.')\n",
    "    new_data.append(new_row)\n",
    "    \n",
    "bindata_blind = np.zeros((1000, len(feature_map)))\n",
    "bindata_blind\n",
    "\n",
    "for i, row in enumerate(new_data):\n",
    "    for x in row:\n",
    "        bindata_blind[i][x] = 1\n",
    "\n",
    "\n",
    "data_blind_tupe = []\n",
    "for row in bindata_blind:\n",
    "    row_vect = np.append(row,[1.]) # ADD STUFF TO VECTOR HERE\n",
    "    data_blind_tupe.append(row_vect)\n",
    "\n",
    "\n",
    "neg = 0\n",
    "match = 0\n",
    "positives = 0\n",
    "labels = []\n",
    "      \n",
    "for i, x in enumerate(data_blind_tupe):\n",
    "         \n",
    "    prediction = np.dot(x,best_weight_avg)  # COULDN'T MAKE IT WORK USING THE TRANSFORMED AVERAGED DATA, DECIDED TO JUST USE AVERAGE WEIGHT VECTOR INSTEAD. STILL 14.7% ACCURACY.\n",
    "            \n",
    "    # POSITIVES\n",
    "    if (prediction > 0):\n",
    "        positives += 1\n",
    "        labels.append('>50K')\n",
    "                \n",
    "                \n",
    "    # NO MATCH\n",
    "    else:\n",
    "        neg += 1\n",
    "        labels.append('<=50K')\n",
    "        \n",
    "        \n",
    "neg = neg/len(data_blind_tupe)*100\n",
    "pos = positives/len(data_blind_tupe)*100\n",
    "\n",
    "\n",
    "print(neg, pos)\n",
    "\n",
    "\n",
    "\n",
    "with open('income.test.predicted.txt', 'w') as file:\n",
    "    for i,row in enumerate(blind):\n",
    "        line = row\n",
    "        line.append(labels[i])\n",
    "        file.write(', '.join(line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
